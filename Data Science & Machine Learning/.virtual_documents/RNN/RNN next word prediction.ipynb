import numpy as np
import torch
import torch.nn.functional as F
import sys


inputString = [2,45,30,55,10]
outString = [25,30,55,10,1]


numFeatures = 100
vocabSize = 80


#define embedding for each input vector of size (100,1) random numbers
embedding = []
for i  in range(len(inputString)):
    a = np.random.rand(numFeatures,1)
    embedding.append(a)
     


len(embedding)


# define the one hot encoding of vocab size
def OneHotEncoding(idx):
    var = np.zeros((vocabSize,1),dtype=float)
    var[idx] = 1
    return var


numUnits = 50
h0 = torch.tensor(np.zeros((numUnits,1)))
Wh = torch.tensor(np.random.uniform(0,1,(numUnits,numUnits)),requires_grad = True)
Wx = torch.tensor(np.random.uniform(0,1,(numUnits,numFeatures)),requires_grad = True)
Wy = torch.tensor(np.random.uniform(0,1,(vocabSize,numUnits)),requires_grad = True)


#define stepforward function input:  xt,wx,wh,wy,prevMemory ; output : ht,y_hat
def stepForward(xt,Wx,Wh,Wy,prevMemory):
    x_frd = torch.matmul(Wx,torch.from_numpy(xt))
    h_frd = torch.matmul(Wh,prevMemory)
    ht = torch.tanh(x_frd + h_frd)
    y_hat = F.softmax(torch.matmul(Wy,ht),dim=0)
    return ht,y_hat


# calculate the y_hat and ht with step_forward
ht,y_hat = stepForward(embedding[0],Wx,Wh,Wy,h0)


ht.shape


y_hat.shape


# define fullforwardpass function ; input :- X,Wx,Wh,Wy,previousMemory ; output: y_hat matrix
def fullForward(X,Wx,Wh,Wy,prevMemory):
    y_hat = []
    for i in range(len(X)):
        ht,yhat = stepForward(X[i],Wx,Wh,Wy,prevMemory)
        prevMemory = ht
        y_hat.append(yhat)
    return y_hat


y_hat = fullForward(embedding,Wx,Wh,Wy,h0)


y_hat[0].shape


# define a computeLoss function that computes the log loss ; input: y,y_hat and output : loss
def computeLoss(y,y_hat):
    loss = 0
    for yi,yi_hat in zip(y,y_hat):
        lt = -torch.log2(yi_hat[yi==1])
        loss += lt
    return loss/len(y)


# get y to compute the loss ; hint : it would be oneHot vector of output string index
y = []
for idx in outString:
    y.append(OneHotEncoding(idx))


print(computeLoss(y,y_hat))


def updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr):
    with torch.no_grad():
        Wx -= lr*dWx
        Wy -= lr*dWy
        Wh -= lr*dWh
    return Wx,Wh,Wy


# define the trainRNN function ; input : X,y,Wx,Wh,Wy,prevMemory ; output : losses and weights
def trainRNN(X,y,Wx,Wh,Wy,prevMemory,lr,nepochs):
    losses = []
    for epoch in range(nepochs):
        y_hat = fullForward(X,Wx,Wh,Wy,prevMemory)
        loss = computeLoss(y,y_hat)
        loss.backward()
        losses.append(loss)
        print("Loss after epoch %d : %f" %(epoch,loss))
        sys.stdout.flush()
        dWx = Wx.grad.data
        dWh = Wh.grad.data
        dWy  = Wy.grad.data
        Wx,Wh,Wy = updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr)
        Wx.grad.data.zero_()
        Wh.grad.data.zero_()
        Wy.grad.data.zero_()
    return Wx,Wh,Wy, losses


Wx,Wh,Wy,losses = trainRNN(embedding,y,Wx,Wh,Wy,h0,0.001,100)



