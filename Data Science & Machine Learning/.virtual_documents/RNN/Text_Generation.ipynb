import tensorflow as tf
import numpy as np
import os


gpus = tf.config.list_physical_devices('GPU')
if gpus:
  # Restrict TensorFlow to only use the first GPU
  try:
    tf.config.set_visible_devices(gpus[0], 'GPU')
    logical_gpus = tf.config.list_logical_devices('GPU')
    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPU")
  except RuntimeError as e:
    # Visible devices must be set before GPUs have been initialized
    print(e)


file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')


file


text = open(file,'rb').read().decode(encoding='utf-8')


type(text)


print(text[:100])


len(text)


len(set(text))


vocab = sorted(set(text))


c2i = {a:i for i,a in enumerate(vocab)}


i2c = np.array(vocab)


texts_as_sequence_of_int = np.array([c2i[a] for a in text])


texts_as_sequence_of_int[:10]


i2c[texts_as_sequence_of_int[:10]]


seq_length = 120
example_per_epochs = len(text)//(seq_length+1)


char_dataset = tf.data.Dataset.from_tensor_slices(texts_as_sequence_of_int)


for i in char_dataset.take(5):
    print(i2c[i.numpy()],i.numpy())


seq = char_dataset.batch(seq_length+1,drop_remainder=True)


def f_make_input_target_pairs(s):
    input_text = s[:-1]
    target_text = s[1:]
    return input_text,target_text


dataset = seq.map(f_make_input_target_pairs)


for X,y in dataset.take(1):
    print(i2c[X.numpy()])
    print(i2c[y.numpy()])


BS = 50
dataset = dataset.shuffle(1000).batch(BS,drop_remainder=True)


VS = len(vocab)
ED = 100
ND = 1024


def f_make_model(VS,ED,ND,BS):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(VS,ED),
        tf.keras.layers.GRU(ND,return_sequences=True,
                           stateful=True),
        tf.keras.layers.Dense(VS)
    ])
    return model


model = f_make_model(VS,ED,ND,BS)


def f_loss(y,y_hat):
    return tf.keras.losses.sparse_categorical_crossentropy(y,y_hat,from_logits=True)


model.compile(optimizer='adam',loss=f_loss)


checkpointDir = './tr_checkpoint'
batch = BS
checkpoint_prefix = os.path.join(checkpointDir,'chpt_50.weights.h5')
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)


history = model.fit(dataset,epochs=15,callbacks=[checkpoint_callback])


model_1 = f_make_model(VS,ED,ND,1)


model_1.load_weights(tf.train.load_checkpoint(checkpoint_callback))


model.build(tf.TensorShape([1,None]))


def f_write_now(model,ss):
    N = 10000
    ie = [c2i[a] for a in ss]
    ie = tf.expand_dims(ie,axis=0)
    g_text = []
    # model.reset_states(stateful=True)
    for i in range(N):
        p = model(ie)
        print(p.shape)
        p = np.squeeze(p,axis=0)
        p_id = tf.random.categorical(p,num_samples=1)[-1,0].numpy()
        ie = tf.expand_dims(p_id,axis=0)
        g_text.append(i2c[p_id])
    retutn (ss + ''.join(g_text))


print(f_write_now(model,ss=u"ROMEO: "))


print(VS)



