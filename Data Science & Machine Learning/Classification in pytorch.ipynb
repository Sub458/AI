{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a73860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3fe2df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "rootdir = '/Data_cfar10'\n",
    "T = datasets.CIFAR10(rootdir,train=True,download=True)\n",
    "V = datasets.CIFAR10(rootdir,train=False,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fe181d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2c85ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc08072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935a883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = T[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cde8239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1AxqMnio4ZILhN8Esci+qMCK5248TR/8ACGPfJuac27DO0kbwMHJ/WvKfDfiS60jXZPsL7ogoIRsgSAjuPTNdrrrTzPPjh3JM988selVrySO3jVnIAZgoz6muJX4mv5TgaUWmjTcwEuF6fnXG694v1XWdsxfyLad4wkUTZ2sMkZ9D1qZYmK2K+qz3aOmivbbTFhtJS8mllmE8OwsTuyBjv1Iri/GL2FnFaXWjtKYlBRjLCVwDyFJ7nOa7aTyHBjkIGB39PWq9/Z2OuWvkMwwrK4444Oa8dVddTojVSVjz7SdE8QTIjI8McMyhhI8gOFPPQc02ytLuLxNHo8zI/wBlk8zeDjcoBI4/EV6Rp9gLGyjghVWWIFeOuO1Z17oYk16LVkC+coRTnuvIP6EUe0TbuP20nof/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c61e243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horse'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.classes[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718f9faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "T = datasets.CIFAR10(rootdir,train=True,download=True,transform=transforms.ToTensor())\n",
    "V = datasets.CIFAR10(rootdir,train=False,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe6beb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = torch.utils.data.DataLoader(T,batch_size=64,shuffle=True,drop_last=True)\n",
    "vl = torch.utils.data.DataLoader(V,batch_size=64,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54181c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(100,10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c627ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d230a479-a71d-4f26-9a02-66302cc64c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7743f9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3072, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (4): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,len(tl))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acb9b260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1845.229248046875\n",
      "1799.1053466796875\n",
      "1799.0830078125\n",
      "1799.028076171875\n",
      "1799.0653076171875\n",
      "1799.0543212890625\n",
      "1799.1715087890625\n",
      "1799.1617431640625\n",
      "1799.1094970703125\n",
      "1799.168212890625\n",
      "1799.1160888671875\n",
      "1799.1197509765625\n",
      "1799.101806640625\n",
      "1798.9989013671875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nepochs):\n\u001b[0;32m      3\u001b[0m     e_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X,y \u001b[38;5;129;01min\u001b[39;00m tl:\n\u001b[0;32m      5\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_dl\\lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_dl\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_dl\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nepochs = 200\n",
    "for e in range(nepochs):\n",
    "    e_loss = 0\n",
    "    for X,y in tl:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size,-1))\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        e_loss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(float(e_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58956f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "c=0\n",
    "with torch.no_grad():\n",
    "    for Xv,yv in vl:\n",
    "        batch_size = Xv.shape[0]\n",
    "        y_hat = model(Xv.view(batch_size,-1))\n",
    "        _,p = torch.max(y_hat,axis=1)\n",
    "        t += int(yv.shape[0])\n",
    "        c += int((p==yv).sum())\n",
    "print(c/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7218ccb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1c681ae5a90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2c39869",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m ,b \u001b[38;5;129;01min\u001b[39;00m vl\n\u001b[0;32m      2\u001b[0m a\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a ,b in vl\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eca1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
