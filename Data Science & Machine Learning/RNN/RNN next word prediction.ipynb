{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a27612-53bc-4772-8714-738bd3cab23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf55170-169a-4c3d-89eb-2e0b522534a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputString = [2,45,30,55,10]\n",
    "outString = [25,30,55,10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26c2023-226d-40b3-9188-c19d91a39ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures = 100\n",
    "vocabSize = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "727f446d-cbbb-4d77-8340-1ce5145b03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define embedding for each input vector of size (100,1) random numbers\n",
    "embedding = []\n",
    "for i  in range(len(inputString)):\n",
    "    a = np.random.rand(numFeatures,1)\n",
    "    embedding.append(a)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd4ebb4-4e6a-42cf-b0ab-0b15f4d27ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562b724d-92a3-4876-8675-6618573b675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the one hot encoding of vocab size\n",
    "def OneHotEncoding(idx):\n",
    "    var = np.zeros((vocabSize,1),dtype=float)\n",
    "    var[idx] = 1\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3001402c-9df8-4ffa-b601-06e6e0dd798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numUnits = 50\n",
    "h0 = torch.tensor(np.zeros((numUnits,1)))\n",
    "Wh = torch.tensor(np.random.uniform(0,1,(numUnits,numUnits)),requires_grad = True)\n",
    "Wx = torch.tensor(np.random.uniform(0,1,(numUnits,numFeatures)),requires_grad = True)\n",
    "Wy = torch.tensor(np.random.uniform(0,1,(vocabSize,numUnits)),requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475ccfef-d90c-4787-8225-a9909a3089b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stepforward function input:  xt,wx,wh,wy,prevMemory ; output : ht,y_hat\n",
    "def stepForward(xt,Wx,Wh,Wy,prevMemory):\n",
    "    x_frd = torch.matmul(Wx,torch.from_numpy(xt))\n",
    "    h_frd = torch.matmul(Wh,prevMemory)\n",
    "    ht = torch.tanh(x_frd + h_frd)\n",
    "    y_hat = F.softmax(torch.matmul(Wy,ht),dim=0)\n",
    "    return ht,y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2301efc3-8785-4955-896a-cd93a8308f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the y_hat and ht with step_forward\n",
    "ht,y_hat = stepForward(embedding[0],Wx,Wh,Wy,h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8da9652e-f106-418f-8e03-c9b4ec9851a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d05a10b-e980-4d3a-a8f1-646a739a34c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1386af9e-8d2e-40e9-8486-5501d5eeca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fullforwardpass function ; input :- X,Wx,Wh,Wy,previousMemory ; output: y_hat matrix\n",
    "def fullForward(X,Wx,Wh,Wy,prevMemory):\n",
    "    y_hat = []\n",
    "    for i in range(len(X)):\n",
    "        ht,yhat = stepForward(X[i],Wx,Wh,Wy,prevMemory)\n",
    "        prevMemory = ht\n",
    "        y_hat.append(yhat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50fbf8d5-3040-47e8-9576-b4d719d6a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = fullForward(embedding,Wx,Wh,Wy,h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed41ed14-5214-4316-874b-159b67f23bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee03118d-8890-48e4-9baf-0f136f0be01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a computeLoss function that computes the log loss ; input: y,y_hat and output : loss\n",
    "def computeLoss(y,y_hat):\n",
    "    loss = 0\n",
    "    for yi,yi_hat in zip(y,y_hat):\n",
    "        lt = -torch.log2(yi_hat[yi==1])\n",
    "        loss += lt\n",
    "    return loss/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cc362be-64d6-41e9-938e-34b8838e1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get y to compute the loss ; hint : it would be oneHot vector of output string index\n",
    "y = []\n",
    "for idx in outString:\n",
    "    y.append(OneHotEncoding(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fa2eada-518d-4b45-ae3f-e97119e0d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9471], dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(computeLoss(y,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f2a7e5b-ccba-47ee-8527-5a145b344e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr):\n",
    "    with torch.no_grad():\n",
    "        Wx -= lr*dWx\n",
    "        Wy -= lr*dWy\n",
    "        Wh -= lr*dWh\n",
    "    return Wx,Wh,Wy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5001080a-5283-45be-b062-b5e8b61753bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the trainRNN function ; input : X,y,Wx,Wh,Wy,prevMemory ; output : losses and weights\n",
    "def trainRNN(X,y,Wx,Wh,Wy,prevMemory,lr,nepochs):\n",
    "    losses = []\n",
    "    for epoch in range(nepochs):\n",
    "        y_hat = fullForward(X,Wx,Wh,Wy,prevMemory)\n",
    "        loss = computeLoss(y,y_hat)\n",
    "        loss.backward()\n",
    "        losses.append(loss)\n",
    "        print(\"Loss after epoch %d : %f\" %(epoch,loss))\n",
    "        sys.stdout.flush()\n",
    "        dWx = Wx.grad.data\n",
    "        dWh = Wh.grad.data\n",
    "        dWy  = Wy.grad.data\n",
    "        Wx,Wh,Wy = updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr)\n",
    "        Wx.grad.data.zero_()\n",
    "        Wh.grad.data.zero_()\n",
    "        Wy.grad.data.zero_()\n",
    "    return Wx,Wh,Wy, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddc9dd11-89b2-4dca-ac20-fd390ca81bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0 : 7.355248\n",
      "Loss after epoch 1 : 7.311033\n",
      "Loss after epoch 2 : 7.289034\n",
      "Loss after epoch 3 : 7.267089\n",
      "Loss after epoch 4 : 7.245198\n",
      "Loss after epoch 5 : 7.223363\n",
      "Loss after epoch 6 : 7.201583\n",
      "Loss after epoch 7 : 7.179859\n",
      "Loss after epoch 8 : 7.158192\n",
      "Loss after epoch 9 : 7.136580\n",
      "Loss after epoch 10 : 7.115026\n",
      "Loss after epoch 11 : 7.093528\n",
      "Loss after epoch 12 : 7.072089\n",
      "Loss after epoch 13 : 7.050707\n",
      "Loss after epoch 14 : 7.029383\n",
      "Loss after epoch 15 : 7.008117\n",
      "Loss after epoch 16 : 6.986911\n",
      "Loss after epoch 17 : 6.965764\n",
      "Loss after epoch 18 : 6.944676\n",
      "Loss after epoch 19 : 6.923648\n",
      "Loss after epoch 20 : 6.902681\n",
      "Loss after epoch 21 : 6.881774\n",
      "Loss after epoch 22 : 6.860928\n",
      "Loss after epoch 23 : 6.840143\n",
      "Loss after epoch 24 : 6.819419\n",
      "Loss after epoch 25 : 6.798758\n",
      "Loss after epoch 26 : 6.778158\n",
      "Loss after epoch 27 : 6.757621\n",
      "Loss after epoch 28 : 6.737147\n",
      "Loss after epoch 29 : 6.716736\n",
      "Loss after epoch 30 : 6.696389\n",
      "Loss after epoch 31 : 6.676105\n",
      "Loss after epoch 32 : 6.655885\n",
      "Loss after epoch 33 : 6.635730\n",
      "Loss after epoch 34 : 6.615639\n",
      "Loss after epoch 35 : 6.595613\n",
      "Loss after epoch 36 : 6.575652\n",
      "Loss after epoch 37 : 6.555757\n",
      "Loss after epoch 38 : 6.535927\n",
      "Loss after epoch 39 : 6.516163\n",
      "Loss after epoch 40 : 6.496466\n",
      "Loss after epoch 41 : 6.476835\n",
      "Loss after epoch 42 : 6.457271\n",
      "Loss after epoch 43 : 6.437775\n",
      "Loss after epoch 44 : 6.418345\n",
      "Loss after epoch 45 : 6.398983\n",
      "Loss after epoch 46 : 6.379689\n",
      "Loss after epoch 47 : 6.360463\n",
      "Loss after epoch 48 : 6.341305\n",
      "Loss after epoch 49 : 6.322215\n",
      "Loss after epoch 50 : 6.303194\n",
      "Loss after epoch 51 : 6.284242\n",
      "Loss after epoch 52 : 6.265360\n",
      "Loss after epoch 53 : 6.246546\n",
      "Loss after epoch 54 : 6.227802\n",
      "Loss after epoch 55 : 6.209127\n",
      "Loss after epoch 56 : 6.190523\n",
      "Loss after epoch 57 : 6.171988\n",
      "Loss after epoch 58 : 6.153524\n",
      "Loss after epoch 59 : 6.135130\n",
      "Loss after epoch 60 : 6.116806\n",
      "Loss after epoch 61 : 6.098553\n",
      "Loss after epoch 62 : 6.080371\n",
      "Loss after epoch 63 : 6.062260\n",
      "Loss after epoch 64 : 6.044220\n",
      "Loss after epoch 65 : 6.026251\n",
      "Loss after epoch 66 : 6.008353\n",
      "Loss after epoch 67 : 5.990526\n",
      "Loss after epoch 68 : 5.972771\n",
      "Loss after epoch 69 : 5.955088\n",
      "Loss after epoch 70 : 5.937476\n",
      "Loss after epoch 71 : 5.919936\n",
      "Loss after epoch 72 : 5.902467\n",
      "Loss after epoch 73 : 5.885071\n",
      "Loss after epoch 74 : 5.867746\n",
      "Loss after epoch 75 : 5.850493\n",
      "Loss after epoch 76 : 5.833312\n",
      "Loss after epoch 77 : 5.816203\n",
      "Loss after epoch 78 : 5.799166\n",
      "Loss after epoch 79 : 5.782202\n",
      "Loss after epoch 80 : 5.765309\n",
      "Loss after epoch 81 : 5.748488\n",
      "Loss after epoch 82 : 5.731739\n",
      "Loss after epoch 83 : 5.715063\n",
      "Loss after epoch 84 : 5.698458\n",
      "Loss after epoch 85 : 5.681925\n",
      "Loss after epoch 86 : 5.665464\n",
      "Loss after epoch 87 : 5.649075\n",
      "Loss after epoch 88 : 5.632758\n",
      "Loss after epoch 89 : 5.616513\n",
      "Loss after epoch 90 : 5.600339\n",
      "Loss after epoch 91 : 5.584238\n",
      "Loss after epoch 92 : 5.568207\n",
      "Loss after epoch 93 : 5.552248\n",
      "Loss after epoch 94 : 5.536361\n",
      "Loss after epoch 95 : 5.520545\n",
      "Loss after epoch 96 : 5.504800\n",
      "Loss after epoch 97 : 5.489126\n",
      "Loss after epoch 98 : 5.473523\n",
      "Loss after epoch 99 : 5.457992\n"
     ]
    }
   ],
   "source": [
    "Wx,Wh,Wy,losses = trainRNN(embedding,y,Wx,Wh,Wy,h0,0.001,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1825d86-c7d5-4837-83ca-d7863245ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
